heat_template_version: 2015-10-15

description: Autoscale (Up with HEAT and Down with internals managed by etcd)

parameters:
  key_name:
    type: string
    label: Key Name
    description: Name of key-pair to be used for compute instance
  image:
    type: string
    label: Image Name
    description: Image to be used for compute instance
    default: Ubuntu Xenial #Note: Currently only works on Xenial and Centos
  instance_type:
    type: string
    label: Instance Type
    description: Type of instance (flavor) to be used
    default: m1.tiny
  net_name:
    type: string
    label: Network Name
    description: Name of the network to use
  OS_USERNAME:
    type: string
    label: Your username
    hidden: true
  OS_PASSWORD:
    type: string
    label: Your password
    hidden: true
  OS_TENANT_NAME:
    type: string
    label: Your tenant name
    hidden: true
  OS_REGION:
    type: string
    label: Your region
    default: "regionOne"
  OS_AUTH_URL:
    type: string
    label: The authorisation url for openstack
    default: "http://delta.internal.sanger.ac.uk/5000/v2.0/"
  sec_grps:
    type: comma_delimited_list
    label: a comma separated list of security groups
    description: The list of security groups your instances require
    default: ["cockroachdb","internal_etcd","cloudforms_ssh_in","netdata"]
  capacity:
    type: number 
    label: start group size
    default: 3 
  min_cluster:
    type: number
    label: minimum cluster size (the point at which heat will autoreplace failed nodes). Note that the scale down scripts will only scale down to capacity, not min_cluster
    default: 3
  max_cluster:
    type: number
    label: maximum cluster size
    default: 5
  scaledownperiod:
    type: number
    label: time between scales (down)
    description: The minimum time between 1 idle instance being removed and the next idle instance being removed
    default: 200
  configscript: 
    type: string
    label: name of script
    description: The name the script should have on the instances
  downmetric:
    type: string
    label: the netdata metric to use for scaling down
    description: the netdata metric to use for scaling down (e.g. `NETDATA_SYSTEM_CPU_IDLE` or `NETDATA_SYSTEM_LIAD_LOAD1` or `NETDATA_SYSTEM_IO_IN, see doc/all_metrics for more options). Currently, this is only as it would be returned, however, I plan to add rate of change
    default: NETDATA_SYSTEM_CPU_IDLE
  threshold:
    type: number
    label: the scaledown threshold of the chosen metric (default is NETDATA_SYSTEM_CPU_IDLE)
    default: 10
  comparator:
    type: string
    label: the comparator between the metric and the threshold
    default: '<'
    constraints:
      - allowed_values:
        - '<'
        - '>'
        - '=='
        - '<='
        - '>='
  etcdclientport:
    type: number
    label: etcd client-side port
    description: the tcp port over which etcd serves client requests
    default: 12379
  etcdpeerport:
    type: number
    label: etcd peer-side port
    description: the tcp port over which etcd communicates with the rest of the cluster
    default: 12380 
  retries:
    type: number
    label: the number of attempt that should be made to join the etcd cluster
    default: 10
  lockattemptperiod:
    type: number
    label: the minimum time between a host's attempts at acquiring the kill lock
    default: 10
  scaleupcooldown:
    type: number
    label: the minimum time between scale up operations
    default: 240
  etcdclientscheme:
    type: string
    label: client protocol (http or https)
    constraints:
      - allowed_values:
        - HTTP
        - HTTPS
    default: HTTP
  etcdpeerscheme:
    type: string
    label: peer to peer protocol (http or https)
    constraints:
      - allowed_values:
        - HTTP
        - HTTPS
    default: HTTP
  proxies:
    type: number
    label: number of proxies
    default: 0
  upmetric:
    type: string
    label: the heat metric to scale up for
    default: cpu_util
  metrics_server:
    type: number
    label: whether or not a metrics server should be made (0 or 1)
    default: 0
    constraints:
      - allowed_values:
        - 0
        - 1
  failtolerance:
    type: number
    label: the failure tolerance of etcd communications
    description: every lockattemptperiod seconds, an etcd communication is made. If this fails, the failmarker goes up by 5, but if it succeeds, the marker goes down by 1. If this marker exceeds failtolerance, the machine is removed
    default: 20
  protocol_port:
    type: number
    label: the port used for external access to your service
    default: 80

resources:
  pool:
    type: OS::Neutron::Pool
    properties:
      lb_method: ROUND_ROBIN
      monitors: 
        - get_resource: monitor
      vip:
        get_param: protocol_port
      protocol: 
        get_param: etcdclientscheme
      
  monitor:
    type: OS::Neutron::HealthMonitor
    properties:
      type: TCP
      delay: 5
      max_retries: 5
      timeout: 5

  lb:
    type: OS::Neutron::LoadBalancer
    properties:
      protocol_port:
        get_param: protocol_port
      pool_id:
        get_resource: pool

  lb_floating:
    type: OS::Neutron::FloatingIP
    properties:
      floating_network: 
        get_param: net_name
      port_id: 
        get_attr: [pool, vip, port_id]

  asg:
    type: OS::Heat::AutoScalingGroup
    properties:
      cooldown: 
        get_param: scaleupcooldown
      desired_capacity: 
        get_param: capacity
      max_size: 
        get_param: max_cluster
      min_size: 
        get_param: min_cluster
      resource:
        type: lb_server.yaml
        properties:
          key_name: 
            get_param: key_name
          image:
            get_param: image
          instance_type:
            get_param: instance_type
          net_name:
            get_param: net_name
          sec_grps:
            get_param: sec_grps
          scaledownperiod:
            get_param: scaledownperiod
          downmetric:
            get_param: downmetric
          comparator:
            get_param: comparator
          threshold:
            get_param: threshold
          etcdclientport:
            get_param: etcdclientport
          lockattemptperiod:
            get_param: lockattemptperiod
          failtolerance:
            get_param: failtolerance
          etcdclientscheme:
            get_param: etcdclientscheme
          OS_USERNAME:
            get_param: OS_USERNAME
          OS_PASSWORD: 
            get_param: OS_PASSWORD
          OS_TENANT_NAME:
            get_param: OS_TENANT_NAME
          OS_REGION:
            get_param: OS_REGION
          capacity:
            get_param: capacity
          configscript: 
            get_param: configscript
          OS_AUTH_URL:
            get_param: OS_AUTH_URL
          etcdpeerport: 
            get_param: etcdpeerport
          retries: 
            get_param: retries
          etcdpeerscheme: 
            get_param: etcdpeerscheme
          pool_id:
            get_resource: pool
          protocol_port:
            get_param: protocol_port
#    type: OS::Heat::AutoScalingGroup
 #   properties:
  #    cooldown: {get_param: scaleupcooldown}
   #   desired_capacity: {get_param: capacity}
    #  max_size: {get_param: max_cluster}
     # min_size: {get_param: min_cluster}
      #resource:
       # type: OS::Nova::Server
        #properties:
          #name: etcd_cluster-node-%index%
         # key_name: { get_param: key_name } #use a pre-registered key for access after creation
          #image: { get_param: image }    #which image to use for the VM
#          config_drive: true
 #         flavor: { get_param: instance_type }      #which flavor of instance to boot
  #        networks: [{"network": { get_param: net_name }}]   #which private network to put the instance on
   #       security_groups: {get_param: sec_grps}
    #      metadata: {"metering.stack": {get_param: "OS::stack_id"}}
     #     personality:
      #      /home/etcd/locking.py: 
       #       str_replace:
        #        template:
         #         get_file: etcd/etcd_locking.py
          #      params:
           #       $thisisatimeout:
            #        get_param: scaledownperiod
             #     $thisisametric:
              #      get_param: downmetric
               #   $thisisacomparator:
                #    get_param: comparator
                 # $thisisathreshold:
                  #  get_param: threshold
#                  $thisisaclientport: 
 #                   get_param: etcdclientport
  #                $thisisanattemptperiod:
   #                 get_param: lockattemptperiod
    #              $thisisatolerance:
     #               get_param: failtolerance
      #      /home/etcd/etcd2.service: 
       #       get_file: etcd/etcd_etcd2.service
        #    /home/etcd/configscript.sh: 
         #     str_replace:
          #      template:
           #       get_file: configscript.sh
            #    params:
             #     $thisisaclientport:
              #      get_param: etcdclientport
               #   $thisisaclientscheme:
                #    get_param: etcdclientscheme  
#            /home/etcd/cleanup.sh:
 #             get_file: cleanupscript.sh
  #          /home/etcd/healthcheck.sh:
   #           str_replace:
    #            template:
     #             get_file: etcd/etcd_healthcheck.sh
      #          params:
       #           $thisisaclientport: {get_param: etcdclientport}
        #          $etcd_client_scheme: {get_param: etcdclientscheme}
         #         $thisisanattemptperiod: {get_param: lockattemptperiod}
          #user_data_format: RAW
#          user_data: 
 #           str_replace:
  #            template: { get_file: etcd/etcd_autoscale.sh }
   #           params:
    #            $thisisausername: {get_param: OS_USERNAME}
     #           $thisisapassword: {get_param: OS_PASSWORD}
      #          $thisisatenantname: {get_param: OS_TENANT_NAME}
       #         $thisisaregion: {get_param: OS_REGION}
        #        $thisisacapacity: {get_param: capacity}
         #       $thisisatimeout: {get_param: scaledownperiod}
          #      $thisisametricserver: { get_attr: [metrics_ip, floating_ip_address] }
           #     $thisisascriptname: {get_param: configscript}
            #    $thisisaurl: {get_param: OS_AUTH_URL}
             #   $thisisapeerport: {get_param: etcdpeerport}
              #  $thisisaclientport: {get_param: etcdclientport}
               # $thisisaretrycount: {get_param: retries}
                #$thisisaclientscheme: {get_param: etcdclientscheme}
#                $thisisapeerscheme: {get_param: etcdpeerscheme}
 #               $thisisaninstanceid: {get_param: "OS::stack_id"}
 
  proxy-group:
    type: OS::Heat::AutoScalingGroup
    properties:
      cooldown: 
        get_param: scaleupcooldown
      desired_capacity: 
        get_param: proxies
      max_size: 
        get_param: proxies
      min_size: 
        get_param: proxies
      resource:
        type: OS::Nova::Server
        properties:
          name: etcd-proxy
          key_name: 
            get_param: key_name  #use a pre-registered key for access after creation
          image: 
            get_param: image     #which image to use for the VM
          config_drive: true
          flavor: 
            get_param: instance_type       #which flavor of instance to boot
          networks: 
            - "network": 
                get_param: net_name    #which private network to put the instance on
          security_groups: 
            get_param: sec_grps
#      metadata: {"metering.stack": {get_param: "OS::stack_name"}}
          personality:
            /home/etcd/etcd2.service:
              get_file: etcd/etcd_etcd2.service
        #/home/ubuntu/locking.py:
         # str_replace:
          #  template:
           #   get_file: etcd/etcd_locking.py
            #params:
             # $thisisatimeout:
              #  get_param: scaledownperiod
          #    $thisisaclientport:
           #     get_param: etcdclientport
            #  $thisisanattemptperiod:
             #   get_param: lockattemptperiod
       # /home/ubuntu/suicide.service:
        #  get_file: etcd/etcd_suicide.service
        #/home/ubuntu/etcd2.service:
         # get_file: etcd/etcd_etcd2.service
            /home/etcd/configscript.sh:
              str_replace:
                template:
                  get_file: proxyconfig.sh
                params:
                  $thisisaclientport:
                    get_param: etcdclientport
#        /home/ubuntu/cleanup.sh:
 #         get_file: cleanupscript.sh

          user_data_format: RAW
          user_data:
            str_replace:
              template: 
                get_file: etcd/etcd_proxy_autoscale.sh 
              params:
                $thisisausername: 
                  get_param: OS_USERNAME
                $thisisapassword: 
                  get_param: OS_PASSWORD
                $thisisatenantname: 
                  get_param: OS_TENANT_NAME
                $thisisaregion: 
                  get_param: OS_REGION
            #$thisisatimeout: {get_param: scaledownperiod}
                $thisisametricserver: 
                  get_attr: [metrics_ip, floating_ip_address] 
            #$thisisascriptname: {get_param: configscript}
                $thisisaurl: 
                  get_param: OS_AUTH_URL
                $thisisapeerport: 
                  get_param: etcdpeerport
                $thisisaclientport: 
                  get_param: etcdclientport
                $thisisaretrycount: 
                  get_param: retries
                $thisisastackname: 
                  get_param: "OS::stack_name"

  scaleup_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: 
        get_resource: asg 
      cooldown: 
        get_param: scaleupcooldown
      scaling_adjustment: 1

  cpu_alarm_high:
    type: OS::Ceilometer::Alarm
    properties:
      meter_name: 
        get_param: upmetric 
      statistic: avg
      period: 1200
      evaluation_periods: 1
      threshold: 50
      query: 
        - 'field': 'resource_metadata.user_metadata.stack'
          'value': {get_param: "OS::stack_id"}
      alarm_actions:
        - get_attr: [scaleup_policy, alarm_url]
      comparison_operator: gt 

#  machine_count_low:
#    type: OS::Ceilometer::Alarm
 #   properties:
  #    meter_name: cpu
   #   statistic: count
    #  period: 700
     # evaluation_periods: 1
      #threshold: {get_param: min_cluster }
#      query: [{'field':'resource_metadata.user_metadata.stack', 'value': {get_param: "OS::stack_id"}}]
 #     alarm_actions:
  #      - {get_attr: [scaleup_policy, alarm_url]}
   #   comparison_operator: lt

  metrics_group:
    type: OS::Heat::AutoScalingGroup
    properties:
      cooldown: 1
      desired_capacity: 
        get_param: metrics_server
      max_size: 
        get_param: metrics_server
      min_size: 
        get_param: metrics_server
      resource:
        type: OS::Nova::Server
        properties:
          name: metrics_server_netdata
          key_name: 
            get_param: key_name  #use a pre-registered key for access after creation
          image: 
            get_param: image     #which image to use for the VM
          config_drive: true
          flavor: 
            get_param: instance_type       #which flavor of instance to boot
          networks: 
            - port: 
                get_resource: metrics_port   #which private network to put the instance on
#          security_groups: ["cloudforms_ssh_in","netdata"]
          user_data_format: RAW
          user_data:
            str_replace:
              template: 
                get_file: netdata_central_setup.sh 
              params:
                $thisisaninstanceid: 
                  get_param: "OS::stack_id"

  metrics_ip:        #create a floating IP available from nova
    type: OS::Neutron::FloatingIP
    properties:
      port_id: 
        get_resource: metrics_port
      floating_network: nova

  metrics_port:
    type: OS::Neutron::Port
    properties:
      network: 
        get_param: net_name 
      security_groups: ["cloudforms_ssh_in","netdata"]

outputs:
  scale_up_url:
    description: >
      This URL is the webhook to scale up the autoscaling group.  You
      can invoke the scale-up operation by doing an HTTP POST to this
      URL; no body nor extra headers are needed.
    value: 
      get_attr: [scaleup_policy, alarm_url]
